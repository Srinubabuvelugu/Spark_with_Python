{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d1967c-aee6-4c63-8942-56c4119e53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark_read_CSV.com\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec252df-8354-4fd3-ae2f-7366806a76e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Builder',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activeSession',\n",
       " '_convert_from_pandas',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_create_dataframe',\n",
       " '_create_from_pandas_with_arrow',\n",
       " '_create_shell_session',\n",
       " '_getActiveSessionOrCreate',\n",
       " '_get_numpy_record_dtype',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedSession',\n",
       " '_jconf',\n",
       " '_jsc',\n",
       " '_jsparkSession',\n",
       " '_jvm',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " 'active',\n",
       " 'addArtifact',\n",
       " 'addArtifacts',\n",
       " 'addTag',\n",
       " 'builder',\n",
       " 'catalog',\n",
       " 'clearTags',\n",
       " 'client',\n",
       " 'conf',\n",
       " 'copyFromLocalToFs',\n",
       " 'createDataFrame',\n",
       " 'getActiveSession',\n",
       " 'getTags',\n",
       " 'interruptAll',\n",
       " 'interruptOperation',\n",
       " 'interruptTag',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'removeTag',\n",
       " 'sparkContext',\n",
       " 'sql',\n",
       " 'stop',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'udf',\n",
       " 'udtf',\n",
       " 'version']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2b11c2-e4d4-4ed0-ba04-4d61b260b484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_df',\n",
       " '_jreader',\n",
       " '_set_opts',\n",
       " '_spark',\n",
       " 'csv',\n",
       " 'format',\n",
       " 'jdbc',\n",
       " 'json',\n",
       " 'load',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orc',\n",
       " 'parquet',\n",
       " 'schema',\n",
       " 'table',\n",
       " 'text']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef8cf35-15c7-47a5-91e6-2a161bd87cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameReader in module pyspark.sql.readwriter object:\n",
      "\n",
      "class DataFrameReader(OptionUtils)\n",
      " |  DataFrameReader(spark: 'SparkSession')\n",
      " |  \n",
      " |  Interface used to load a :class:`DataFrame` from external storage systems\n",
      " |  (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n",
      " |  to access this.\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  .. versionchanged:: 3.4.0\n",
      " |      Supports Spark Connect.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrameReader\n",
      " |      OptionUtils\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, spark: 'SparkSession')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  csv(self, path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame'\n",
      " |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      " |      \n",
      " |      This function will go through the input once to determine the input schema if\n",
      " |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      " |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list\n",
      " |          string, or list of strings, for input path(s),\n",
      " |          or RDD of Strings storing CSV rows.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a CSV file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file\n",
      " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      " |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      " |      ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |100|NULL|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  format(self, source: str) -> 'DataFrameReader'\n",
      " |      Specifies the input data source format.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      source : str\n",
      " |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.read.format('json')\n",
      " |      <...readwriter.DataFrameReader object ...>\n",
      " |      \n",
      " |      Write a DataFrame into a JSON file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a JSON file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the JSON file as a DataFrame.\n",
      " |      ...     spark.read.format('json').load(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  jdbc(self, url: str, table: str, column: Optional[str] = None, lowerBound: Union[str, int, NoneType] = None, upperBound: Union[str, int, NoneType] = None, numPartitions: Optional[int] = None, predicates: Optional[List[str]] = None, properties: Optional[Dict[str, str]] = None) -> 'DataFrame'\n",
      " |      Construct a :class:`DataFrame` representing the database table named ``table``\n",
      " |      accessible via JDBC URL ``url`` and connection ``properties``.\n",
      " |      \n",
      " |      Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      " |      ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
      " |      is needed when ``column`` is specified.\n",
      " |      \n",
      " |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      table : str\n",
      " |          the name of the table\n",
      " |      column : str, optional\n",
      " |          alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      predicates : list, optional\n",
      " |          a list of expressions suitable for inclusion in WHERE clauses;\n",
      " |          each one defines one partition of the :class:`DataFrame`\n",
      " |      properties : dict, optional\n",
      " |          a dictionary of JDBC database connection arguments. Normally at\n",
      " |          least properties \"user\" and \"password\" with their corresponding values.\n",
      " |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Don't create too many partitions in parallel on a large cluster;\n",
      " |      otherwise Spark might crash your external database systems.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |  \n",
      " |  json(self, path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      " |      Loads JSON files and returns the results as a :class:`DataFrame`.\n",
      " |      \n",
      " |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
      " |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
      " |      \n",
      " |      If the ``schema`` parameter is not specified, this function goes\n",
      " |      through the input once to determine the input schema.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, list or :class:`RDD`\n",
      " |          string represents path to the JSON dataset, or a list of paths,\n",
      " |          or RDD of Strings storing JSON objects.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
      " |          a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a JSON file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a JSON file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the JSON file as a DataFrame.\n",
      " |      ...     spark.read.json(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  load(self, path: Union[str, List[str], NoneType] = None, format: Optional[str] = None, schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
      " |      Loads data from a data source and returns it as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list, optional\n",
      " |          optional string or a list of string for file-system backed data sources.\n",
      " |      format : str, optional\n",
      " |          optional string for format of the data source. Default to 'parquet'.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Load a CSV file with format, schema and options specified.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file with a header\n",
      " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      " |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
      " |      ...     # and 'header' option set to `True`.\n",
      " |      ...     df = spark.read.load(\n",
      " |      ...         d, schema=df.schema, format=\"csv\", nullValue=\"Hyukjin Kwon\", header=True)\n",
      " |      ...     df.printSchema()\n",
      " |      ...     df.show()\n",
      " |      root\n",
      " |       |-- age: long (nullable = true)\n",
      " |       |-- name: string (nullable = true)\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |100|NULL|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
      " |      Adds an input option for the underlying data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          The key for the option to set.\n",
      " |      value\n",
      " |          The value for the option to set.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.read.option(\"key\", \"value\")\n",
      " |      <...readwriter.DataFrameReader object ...>\n",
      " |      \n",
      " |      Specify the option 'nullValue' with reading a CSV file.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file\n",
      " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      " |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      " |      ...     spark.read.schema(df.schema).option(\n",
      " |      ...         \"nullValue\", \"Hyukjin Kwon\").format('csv').load(d).show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |100|NULL|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
      " |      Adds input options for the underlying data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **options : dict\n",
      " |          The dictionary of string keys and prmitive-type values.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.read.option(\"key\", \"value\")\n",
      " |      <...readwriter.DataFrameReader object ...>\n",
      " |      \n",
      " |      Specify the option 'nullValue' and 'header' with reading a CSV file.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file with a header.\n",
      " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      " |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
      " |      ...     # and 'header' option set to `True`.\n",
      " |      ...     spark.read.options(\n",
      " |      ...         nullValue=\"Hyukjin Kwon\",\n",
      " |      ...         header=True\n",
      " |      ...     ).format('csv').load(d).show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |100|NULL|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  orc(self, path: Union[str, List[str]], mergeSchema: Optional[bool] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      " |      Loads ORC files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a ORC file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a ORC file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"orc\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the Parquet file as a DataFrame.\n",
      " |      ...     spark.read.orc(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  parquet(self, *paths: str, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
      " |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      paths : str\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a Parquet file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a Parquet file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the Parquet file as a DataFrame.\n",
      " |      ...     spark.read.parquet(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  schema(self, schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrameReader'\n",
      " |      Specifies the input schema.\n",
      " |      \n",
      " |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n",
      " |      By specifying the schema here, the underlying data source can skip the schema\n",
      " |      inference step, and thus speed up data loading.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str\n",
      " |          a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n",
      " |          (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.read.schema(\"col0 INT, col1 DOUBLE\")\n",
      " |      <...readwriter.DataFrameReader object ...>\n",
      " |      \n",
      " |      Specify the schema with reading a CSV file.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     spark.read.schema(\"col0 INT, col1 DOUBLE\").format(\"csv\").load(d).printSchema()\n",
      " |      root\n",
      " |       |-- col0: integer (nullable = true)\n",
      " |       |-- col1: double (nullable = true)\n",
      " |  \n",
      " |  table(self, tableName: str) -> 'DataFrame'\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tableName : str\n",
      " |          string, name of the table.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.createOrReplaceTempView('tblA')\n",
      " |      >>> spark.read.table('tblA').show()\n",
      " |      +---+\n",
      " |      | id|\n",
      " |      +---+\n",
      " |      |  0|\n",
      " |      |  1|\n",
      " |      |  2|\n",
      " |      |  3|\n",
      " |      |  4|\n",
      " |      |  5|\n",
      " |      |  6|\n",
      " |      |  7|\n",
      " |      |  8|\n",
      " |      |  9|\n",
      " |      +---+\n",
      " |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
      " |  \n",
      " |  text(self, paths: Union[str, List[str]], wholetext: bool = False, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      " |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      " |      string column named \"value\", and followed by partitioned columns if there\n",
      " |      are any.\n",
      " |      The text files must be encoded as UTF-8.\n",
      " |      \n",
      " |      By default, each line in the text file is a new row in the resulting DataFrame.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      paths : str or list\n",
      " |          string, or list of strings, for input path(s).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a text file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a text file\n",
      " |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
      " |      ...     df.write.mode(\"overwrite\").format(\"text\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the text file as a DataFrame.\n",
      " |      ...     spark.read.schema(df.schema).text(d).sort(\"alphabets\").show()\n",
      " |      +---------+\n",
      " |      |alphabets|\n",
      " |      +---------+\n",
      " |      |        a|\n",
      " |      |        b|\n",
      " |      |        c|\n",
      " |      +---------+\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OptionUtils:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8984736e-c139-4fcd-bbc8-59bbda020a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read single CSV file\n",
    "df = spark.read.csv(path = \"Iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93a998cf-9c72-485d-a774-0e4558e49e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|_c0|          _c1|         _c2|          _c3|         _c4|        _c5|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n",
      "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n",
      "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n",
      "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n",
      "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n",
      "| 11|          5.4|         3.7|          1.5|         0.2|Iris-setosa|\n",
      "| 12|          4.8|         3.4|          1.6|         0.2|Iris-setosa|\n",
      "| 13|          4.8|         3.0|          1.4|         0.1|Iris-setosa|\n",
      "| 14|          4.3|         3.0|          1.1|         0.1|Iris-setosa|\n",
      "| 15|          5.8|         4.0|          1.2|         0.2|Iris-setosa|\n",
      "| 16|          5.7|         4.4|          1.5|         0.4|Iris-setosa|\n",
      "| 17|          5.4|         3.9|          1.3|         0.4|Iris-setosa|\n",
      "| 18|          5.1|         3.5|          1.4|         0.3|Iris-setosa|\n",
      "| 19|          5.7|         3.8|          1.7|         0.3|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aab9450-cbe9-4190-87bd-9e4d17c6f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Id: string, SepalLengthCm: string, SepalWidthCm: string, PetalLengthCm: string, PetalWidthCm: string, Species: string]\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- SepalLengthCm: string (nullable = true)\n",
      " |-- SepalWidthCm: string (nullable = true)\n",
      " |-- PetalLengthCm: string (nullable = true)\n",
      " |-- PetalWidthCm: string (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n",
      "None\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n",
      "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n",
      "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n",
      "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n",
      "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n",
      "| 11|          5.4|         3.7|          1.5|         0.2|Iris-setosa|\n",
      "| 12|          4.8|         3.4|          1.6|         0.2|Iris-setosa|\n",
      "| 13|          4.8|         3.0|          1.4|         0.1|Iris-setosa|\n",
      "| 14|          4.3|         3.0|          1.1|         0.1|Iris-setosa|\n",
      "| 15|          5.8|         4.0|          1.2|         0.2|Iris-setosa|\n",
      "| 16|          5.7|         4.4|          1.5|         0.4|Iris-setosa|\n",
      "| 17|          5.4|         3.9|          1.3|         0.4|Iris-setosa|\n",
      "| 18|          5.1|         3.5|          1.4|         0.3|Iris-setosa|\n",
      "| 19|          5.7|         3.8|          1.7|         0.3|Iris-setosa|\n",
      "| 20|          5.1|         3.8|          1.5|         0.3|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(path = \"Iris.csv\",header = True)\n",
    "print(df)\n",
    "print(df.printSchema())\n",
    "print(df.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dbc0a5c-9dae-4b79-b128-1ff5eade155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- SepalLengthCm: float (nullable = true)\n",
      " |-- SepalWidthCm: integer (nullable = true)\n",
      " |-- PetalLengthCm: integer (nullable = true)\n",
      " |-- PetalWidthCm: integer (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# change the datatype of columns\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType().add(field='Id',data_type=IntegerType()) \\\n",
    "                     .add(field='SepalLengthCm',data_type=FloatType()) \\\n",
    "                    .add(field='SepalWidthCm',data_type=IntegerType()) \\\n",
    "                    .add(field='PetalLengthCm',data_type=IntegerType()) \\\n",
    "                    .add(field='PetalWidthCm',data_type=IntegerType()) \\\n",
    "                    .add(field='Species', data_type = StringType())\n",
    "df = spark.read.csv(\"Iris.csv\",header= True,schema=schema)\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "411f70bb-a276-4fa5-8be7-122782135824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|_c0|          _c1|         _c2|          _c3|         _c4|        _c5|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n",
      "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n",
      "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n",
      "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n",
      "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n",
      "| 11|          5.4|         3.7|          1.5|         0.2|Iris-setosa|\n",
      "| 12|          4.8|         3.4|          1.6|         0.2|Iris-setosa|\n",
      "| 13|          4.8|         3.0|          1.4|         0.1|Iris-setosa|\n",
      "| 14|          4.3|         3.0|          1.1|         0.1|Iris-setosa|\n",
      "| 15|          5.8|         4.0|          1.2|         0.2|Iris-setosa|\n",
      "| 16|          5.7|         4.4|          1.5|         0.4|Iris-setosa|\n",
      "| 17|          5.4|         3.9|          1.3|         0.4|Iris-setosa|\n",
      "| 18|          5.1|         3.5|          1.4|         0.3|Iris-setosa|\n",
      "| 19|          5.7|         3.8|          1.7|         0.3|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Another way to read CSV files by using the format-load method\n",
    "df = spark.read.format('csv').load(path= \"Iris.csv\")\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1f4a3db-8b7a-41be-8d39-227c82bac154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n",
      "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n",
      "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n",
      "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n",
      "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n",
      "| 11|          5.4|         3.7|          1.5|         0.2|Iris-setosa|\n",
      "| 12|          4.8|         3.4|          1.6|         0.2|Iris-setosa|\n",
      "| 13|          4.8|         3.0|          1.4|         0.1|Iris-setosa|\n",
      "| 14|          4.3|         3.0|          1.1|         0.1|Iris-setosa|\n",
      "| 15|          5.8|         4.0|          1.2|         0.2|Iris-setosa|\n",
      "| 16|          5.7|         4.4|          1.5|         0.4|Iris-setosa|\n",
      "| 17|          5.4|         3.9|          1.3|         0.4|Iris-setosa|\n",
      "| 18|          5.1|         3.5|          1.4|         0.3|Iris-setosa|\n",
      "| 19|          5.7|         3.8|          1.7|         0.3|Iris-setosa|\n",
      "| 20|          5.1|         3.8|          1.5|         0.3|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- SepalLengthCm: string (nullable = true)\n",
      " |-- SepalWidthCm: string (nullable = true)\n",
      " |-- PetalLengthCm: string (nullable = true)\n",
      " |-- PetalWidthCm: string (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option(key='header',value=True).load(path= \"Iris.csv\")\n",
    "print(df.show())\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64571ac1-a013-4561-a259-2f099ff39f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[App: string, Category: string, Rating: string, Reviews: string, Size: string, Installs: string, Type: string, Price: string, Content Rating: string, Genres: string, Last Updated: string, Current Ver: string, Android Ver: string]\n",
      "root\n",
      " |-- App: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Rating: string (nullable = true)\n",
      " |-- Reviews: string (nullable = true)\n",
      " |-- Size: string (nullable = true)\n",
      " |-- Installs: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Content Rating: string (nullable = true)\n",
      " |-- Genres: string (nullable = true)\n",
      " |-- Last Updated: string (nullable = true)\n",
      " |-- Current Ver: string (nullable = true)\n",
      " |-- Android Ver: string (nullable = true)\n",
      "\n",
      "None\n",
      "+------------------------------------------------------------------------------------------------------------+-----------------+------+-------+------------------+-----------+----+-----+--------------+-------------------------------+------------------+------------------+------------------+\n",
      "|App                                                                                                         |Category         |Rating|Reviews|Size              |Installs   |Type|Price|Content Rating|Genres                         |Last Updated      |Current Ver       |Android Ver       |\n",
      "+------------------------------------------------------------------------------------------------------------+-----------------+------+-------+------------------+-----------+----+-----+--------------+-------------------------------+------------------+------------------+------------------+\n",
      "|Photo Editor & Candy Camera & Grid & ScrapBook                                                              |ART_AND_DESIGN   |4.1   |159    |19M               |10,000+    |Free|0    |Everyone      |Art & Design                   |January 7, 2018   |1.0.0             |4.0.3 and up      |\n",
      "|Coloring book moana                                                                                         |ART_AND_DESIGN   |3.9   |967    |14M               |500,000+   |Free|0    |Everyone      |Art & Design;Pretend Play      |January 15, 2018  |2.0.0             |4.0.3 and up      |\n",
      "|U Launcher Lite â€“ FREE Live Cool Themes, Hide Apps                                                          |ART_AND_DESIGN   |4.7   |87510  |8.7M              |5,000,000+ |Free|0    |Everyone      |Art & Design                   |August 1, 2018    |1.2.4             |4.0.3 and up      |\n",
      "|Sketch - Draw & Paint                                                                                       |ART_AND_DESIGN   |4.5   |215644 |25M               |50,000,000+|Free|0    |Teen          |Art & Design                   |June 8, 2018      |Varies with device|4.2 and up        |\n",
      "|Pixel Draw - Number Art Coloring Book                                                                       |ART_AND_DESIGN   |4.3   |967    |2.8M              |100,000+   |Free|0    |Everyone      |Art & Design;Creativity        |June 20, 2018     |1.1               |4.4 and up        |\n",
      "|Paper flowers instructions                                                                                  |ART_AND_DESIGN   |4.4   |167    |5.6M              |50,000+    |Free|0    |Everyone      |Art & Design                   |March 26, 2017    |1.0               |2.3 and up        |\n",
      "|Smoke Effect Photo Maker - Smoke Editor                                                                     |ART_AND_DESIGN   |3.8   |178    |19M               |50,000+    |Free|0    |Everyone      |Art & Design                   |April 26, 2018    |1.1               |4.0.3 and up      |\n",
      "|Infinite Painter                                                                                            |ART_AND_DESIGN   |4.1   |36815  |29M               |1,000,000+ |Free|0    |Everyone      |Art & Design                   |June 14, 2018     |6.1.61.1          |4.2 and up        |\n",
      "|Garden Coloring Book                                                                                        |ART_AND_DESIGN   |4.4   |13791  |33M               |1,000,000+ |Free|0    |Everyone      |Art & Design                   |September 20, 2017|2.9.2             |3.0 and up        |\n",
      "|Kids Paint Free - Drawing Fun                                                                               |ART_AND_DESIGN   |4.7   |121    |3.1M              |10,000+    |Free|0    |Everyone      |Art & Design;Creativity        |July 3, 2018      |2.8               |4.0.3 and up      |\n",
      "|Text on Photo - Fonteee                                                                                     |ART_AND_DESIGN   |4.4   |13880  |28M               |1,000,000+ |Free|0    |Everyone      |Art & Design                   |October 27, 2017  |1.0.4             |4.1 and up        |\n",
      "|Name Art Photo Editor - Focus n Filters                                                                     |ART_AND_DESIGN   |4.4   |8788   |12M               |1,000,000+ |Free|0    |Everyone      |Art & Design                   |July 31, 2018     |1.0.15            |4.0 and up        |\n",
      "|Tattoo Name On My Photo Editor                                                                              |ART_AND_DESIGN   |4.2   |44829  |20M               |10,000,000+|Free|0    |Teen          |Art & Design                   |April 2, 2018     |3.8               |4.1 and up        |\n",
      "|Mandala Coloring Book                                                                                       |ART_AND_DESIGN   |4.6   |4326   |21M               |100,000+   |Free|0    |Everyone      |Art & Design                   |June 26, 2018     |1.0.4             |4.4 and up        |\n",
      "|3D Color Pixel by Number - Sandbox Art Coloring                                                             |ART_AND_DESIGN   |4.4   |1518   |37M               |100,000+   |Free|0    |Everyone      |Art & Design                   |August 3, 2018    |1.2.3             |2.3 and up        |\n",
      "|Learn To Draw Kawaii Characters                                                                             |ART_AND_DESIGN   |3.2   |55     |2.7M              |5,000+     |Free|0    |Everyone      |Art & Design                   |June 6, 2018      |NULL              |4.2 and up        |\n",
      "|Photo Designer - Write your name with shapes                                                                |ART_AND_DESIGN   |4.7   |3632   |5.5M              |500,000+   |Free|0    |Everyone      |Art & Design                   |July 31, 2018     |3.1               |4.1 and up        |\n",
      "|350 Diy Room Decor Ideas                                                                                    |ART_AND_DESIGN   |4.5   |27     |17M               |10,000+    |Free|0    |Everyone      |Art & Design                   |November 7, 2017  |1.0               |2.3 and up        |\n",
      "|FlipaClip - Cartoon animation                                                                               |ART_AND_DESIGN   |4.3   |194216 |39M               |5,000,000+ |Free|0    |Everyone      |Art & Design                   |August 3, 2018    |2.2.5             |4.0.3 and up      |\n",
      "|ibis Paint X                                                                                                |ART_AND_DESIGN   |4.6   |224399 |31M               |10,000,000+|Free|0    |Everyone      |Art & Design                   |July 30, 2018     |5.5.4             |4.1 and up        |\n",
      "|Logo Maker - Small Business                                                                                 |ART_AND_DESIGN   |4.0   |450    |14M               |100,000+   |Free|0    |Everyone      |Art & Design                   |April 20, 2018    |4.0               |4.1 and up        |\n",
      "|Boys Photo Editor - Six Pack & Men's Suit                                                                   |ART_AND_DESIGN   |4.1   |654    |12M               |100,000+   |Free|0    |Everyone      |Art & Design                   |March 20, 2018    |1.1               |4.0.3 and up      |\n",
      "|Superheroes Wallpapers | 4K Backgrounds                                                                     |ART_AND_DESIGN   |4.7   |7699   |4.2M              |500,000+   |Free|0    |Everyone 10+  |Art & Design                   |July 12, 2018     |2.2.6.2           |4.0.3 and up      |\n",
      "|Mcqueen Coloring pages                                                                                      |ART_AND_DESIGN   |NULL  |61     |7.0M              |100,000+   |Free|0    |Everyone      |Art & Design;Action & Adventure|March 7, 2018     |1.0.0             |4.1 and up        |\n",
      "|HD Mickey Minnie Wallpapers                                                                                 |ART_AND_DESIGN   |4.7   |118    |23M               |50,000+    |Free|0    |Everyone      |Art & Design                   |July 7, 2018      |1.1.3             |4.1 and up        |\n",
      "|Harley Quinn wallpapers HD                                                                                  |ART_AND_DESIGN   |4.8   |192    |6.0M              |10,000+    |Free|0    |Everyone      |Art & Design                   |April 25, 2018    |1.5               |3.0 and up        |\n",
      "|Colorfit - Drawing & Coloring                                                                               |ART_AND_DESIGN   |4.7   |20260  |25M               |500,000+   |Free|0    |Everyone      |Art & Design;Creativity        |October 11, 2017  |1.0.8             |4.0.3 and up      |\n",
      "|Animated Photo Editor                                                                                       |ART_AND_DESIGN   |4.1   |203    |6.1M              |100,000+   |Free|0    |Everyone      |Art & Design                   |March 21, 2018    |1.03              |4.0.3 and up      |\n",
      "|Pencil Sketch Drawing                                                                                       |ART_AND_DESIGN   |3.9   |136    |4.6M              |10,000+    |Free|0    |Everyone      |Art & Design                   |July 12, 2018     |6.0               |2.3 and up        |\n",
      "|Easy Realistic Drawing Tutorial                                                                             |ART_AND_DESIGN   |4.1   |223    |4.2M              |100,000+   |Free|0    |Everyone      |Art & Design                   |August 22, 2017   |1.0               |2.3 and up        |\n",
      "|Pink Silver Bow Keyboard Theme                                                                              |ART_AND_DESIGN   |4.2   |1120   |9.2M              |100,000+   |Free|0    |Everyone      |Art & Design                   |July 12, 2018     |6.7.12.2018       |4.0.3 and up      |\n",
      "|Art Drawing Ideas                                                                                           |ART_AND_DESIGN   |4.1   |227    |5.2M              |50,000+    |Free|0    |Everyone      |Art & Design                   |May 31, 2018      |1.2               |2.3 and up        |\n",
      "|Anime Manga Coloring Book                                                                                   |ART_AND_DESIGN   |4.5   |5035   |11M               |100,000+   |Free|0    |Everyone      |Art & Design                   |July 19, 2018     |2.20              |4.0 and up        |\n",
      "|Easy Origami Ideas                                                                                          |ART_AND_DESIGN   |4.2   |1015   |11M               |100,000+   |Free|0    |Everyone      |Art & Design                   |January 6, 2018   |1.1.0             |4.1 and up        |\n",
      "|I Creative Idea                                                                                             |ART_AND_DESIGN   |4.7   |353    |4.2M              |10,000+    |Free|0    |Teen          |Art & Design                   |April 27, 2018    |1.6               |4.1 and up        |\n",
      "|How to draw Ladybug and Cat Noir                                                                            |ART_AND_DESIGN   |3.8   |564    |9.2M              |100,000+   |Free|0    |Everyone      |Art & Design                   |July 11, 2018     |2.1               |4.1 and up        |\n",
      "|UNICORN - Color By Number & Pixel Art Coloring                                                              |ART_AND_DESIGN   |4.7   |8145   |24M               |500,000+   |Free|0    |Everyone      |Art & Design;Creativity        |August 2, 2018    |1.0.9             |4.4 and up        |\n",
      "|Floor Plan Creator                                                                                          |ART_AND_DESIGN   |4.1   |36639  |Varies with device|5,000,000+ |Free|0    |Everyone      |Art & Design                   |July 14, 2018     |Varies with device|2.3.3 and up      |\n",
      "|PIP Camera - PIP Collage Maker                                                                              |ART_AND_DESIGN   |4.7   |158    |11M               |10,000+    |Free|0    |Everyone      |Art & Design                   |November 29, 2017 |1.3               |4.0.3 and up      |\n",
      "|How To Color Disney Princess - Coloring Pages                                                               |ART_AND_DESIGN   |4.0   |591    |9.4M              |500,000+   |Free|0    |Everyone      |Art & Design                   |March 31, 2018    |1                 |4.0 and up        |\n",
      "|Drawing Clothes Fashion Ideas                                                                               |ART_AND_DESIGN   |4.2   |117    |15M               |10,000+    |Free|0    |Everyone      |Art & Design                   |July 20, 2018     |2.0.1             |4.0.3 and up      |\n",
      "|Sad Poetry Photo Frames 2018                                                                                |ART_AND_DESIGN   |4.5   |176    |10M               |100,000+   |Free|0    |Everyone      |Art & Design                   |April 2, 2018     |1.0               |4.0.3 and up      |\n",
      "|Textgram - write on photos                                                                                  |ART_AND_DESIGN   |4.4   |295221 |Varies with device|10,000,000+|Free|0    |Everyone      |Art & Design                   |July 30, 2018     |Varies with device|Varies with device|\n",
      "|Paint Splash!                                                                                               |ART_AND_DESIGN   |3.8   |2206   |1.2M              |100,000+   |Free|0    |Everyone      |Art & Design;Creativity        |April 15, 2018    |1.46              |4.1 and up        |\n",
      "|Popsicle Sticks and Similar DIY Craft Ideas                                                                 |ART_AND_DESIGN   |4.2   |26     |12M               |10,000+    |Free|0    |Everyone      |Art & Design                   |January 3, 2018   |1.0.0             |4.1 and up        |\n",
      "|Canva: Poster, banner, card maker & graphic design                                                          |ART_AND_DESIGN   |4.7   |174531 |24M               |10,000,000+|Free|0    |Everyone      |Art & Design                   |July 31, 2018     |1.6.1             |4.1 and up        |\n",
      "|Install images with music to make video without Net - 2018                                                  |ART_AND_DESIGN   |4.6   |1070   |26M               |100,000+   |Free|0    |Everyone      |Art & Design                   |November 14, 2017 |1.6               |4.1 and up        |\n",
      "|Little Teddy Bear Colouring Book Game                                                                       |ART_AND_DESIGN   |4.2   |85     |8.0M              |100,000+   |Free|0    |Everyone      |Art & Design                   |December 17, 2017 |2.0.0             |4.1 and up        |\n",
      "|How To Draw Food                                                                                            |ART_AND_DESIGN   |4.3   |845    |7.9M              |100,000+   |Free|0    |Everyone      |Art & Design                   |May 28, 2018      |1.0               |2.3 and up        |\n",
      "|Monster Truck Stunt 3D 2019                                                                                 |AUTO_AND_VEHICLES|4.2   |367    |25M               |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |May 10, 2018      |1.0               |4.0.3 and up      |\n",
      "|Real Tractor Farming                                                                                        |AUTO_AND_VEHICLES|4.0   |1598   |56M               |1,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |July 26, 2018     |11.0              |4.1 and up        |\n",
      "|Ultimate F1 Racing Championship                                                                             |AUTO_AND_VEHICLES|3.8   |284    |57M               |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |July 26, 2018     |3.0               |4.1 and up        |\n",
      "|Used Cars and Trucks for Sale                                                                               |AUTO_AND_VEHICLES|4.6   |17057  |Varies with device|1,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |July 30, 2018     |Varies with device|Varies with device|\n",
      "|American Muscle Car Race                                                                                    |AUTO_AND_VEHICLES|3.9   |129    |35M               |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |July 26, 2018     |3.0               |4.1 and up        |\n",
      "|Offroad Oil Tanker Driver Transport Truck 2019                                                              |AUTO_AND_VEHICLES|4.3   |542    |33M               |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |July 31, 2018     |4.0               |4.1 and up        |\n",
      "|Tickets SDA 2018 and Exam from the State Traffic Safety Inspectorate with Drom.ru                           |AUTO_AND_VEHICLES|4.9   |10479  |33M               |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |July 18, 2018     |1.7.1             |4.0 and up        |\n",
      "|Gas Prices (Germany only)                                                                                   |AUTO_AND_VEHICLES|4.4   |805    |5.6M              |50,000+    |Free|0    |Everyone      |Auto & Vehicles                |July 29, 2018     |2.5.1             |4.4 and up        |\n",
      "|Extreme Rally Championship                                                                                  |AUTO_AND_VEHICLES|4.2   |129    |54M               |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |July 26, 2018     |3.0               |4.1 and up        |\n",
      "|Restart Navigator                                                                                           |AUTO_AND_VEHICLES|4.0   |1403   |201k              |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |August 26, 2014   |1.0.1             |2.2 and up        |\n",
      "|REG - Check the regnumber, find information about Swedish vehicles                                          |AUTO_AND_VEHICLES|3.9   |3971   |3.6M              |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |July 30, 2018     |2.493             |4.4 and up        |\n",
      "|CityBus Lviv                                                                                                |AUTO_AND_VEHICLES|4.6   |534    |5.7M              |10,000+    |Free|0    |Everyone      |Auto & Vehicles                |July 8, 2018      |1.9.1             |4.0.3 and up      |\n",
      "|CDL Practice Test 2018 Edition                                                                              |AUTO_AND_VEHICLES|4.9   |7774   |17M               |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |July 3, 2018      |1.7               |4.2 and up        |\n",
      "|ezETC (ETC balance inquiry, meter trial, real-time traffic)                                                 |AUTO_AND_VEHICLES|4.3   |38846  |8.6M              |1,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |July 28, 2018     |2.20 Build 02     |4.1 and up        |\n",
      "|Free VIN Report for Used Cars                                                                               |AUTO_AND_VEHICLES|4.6   |2431   |2.4M              |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |June 12, 2018     |1.37              |4.0 and up        |\n",
      "|DMV Permit Practice Test 2018 Edition                                                                       |AUTO_AND_VEHICLES|4.9   |6090   |27M               |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |July 3, 2018      |1.7               |4.2 and up        |\n",
      "|Check Vehicle Tax                                                                                           |AUTO_AND_VEHICLES|3.9   |295    |2.7M              |10,000+    |Free|0    |Everyone      |Auto & Vehicles                |July 30, 2018     |0.2.1             |4.4 and up        |\n",
      "|Used Cars Mexico                                                                                            |AUTO_AND_VEHICLES|4.0   |190    |2.5M              |50,000+    |Free|0    |Everyone      |Auto & Vehicles                |October 14, 2016  |1.0               |2.3 and up        |\n",
      "|Ulysse Speedometer                                                                                          |AUTO_AND_VEHICLES|4.3   |40211  |Varies with device|5,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |July 30, 2018     |Varies with device|Varies with device|\n",
      "|REPUVE                                                                                                      |AUTO_AND_VEHICLES|3.9   |356    |Varies with device|100,000+   |Free|0    |Everyone      |Auto & Vehicles                |May 25, 2018      |Varies with device|Varies with device|\n",
      "|Used cars for sale - Trovit                                                                                 |AUTO_AND_VEHICLES|4.2   |52530  |7.0M              |5,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |July 16, 2018     |4.47.3            |4.0.3 and up      |\n",
      "|Fines of the State Traffic Safety Inspectorate are official: inspection, payment of fines                   |AUTO_AND_VEHICLES|4.8   |116986 |35M               |5,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |August 2, 2018    |1.9.7             |4.0.3 and up      |\n",
      "|SK Enca Direct Malls - Used Cars Search                                                                     |AUTO_AND_VEHICLES|3.6   |1379   |16M               |500,000+   |Free|0    |Everyone      |Auto & Vehicles                |August 2, 2018    |2.2.21            |4.2 and up        |\n",
      "|Android Auto - Maps, Media, Messaging & Voice                                                               |AUTO_AND_VEHICLES|4.2   |271920 |16M               |10,000,000+|Free|0    |Teen          |Auto & Vehicles                |July 11, 2018     |Varies with device|5.0 and up        |\n",
      "|PDD-UA                                                                                                      |AUTO_AND_VEHICLES|4.8   |736    |Varies with device|100,000+   |Free|0    |Everyone      |Auto & Vehicles                |July 29, 2018     |2.9               |2.3.3 and up      |\n",
      "|Tickets SDA 2019 + Exam RF                                                                                  |AUTO_AND_VEHICLES|4.8   |7021   |17M               |500,000+   |Free|0    |Everyone      |Auto & Vehicles                |July 27, 2018     |1.79              |4.0 and up        |\n",
      "|Super Cars Wallpapers And Backgrounds                                                                       |AUTO_AND_VEHICLES|4.6   |197    |3.4M              |50,000+    |Free|0    |Everyone      |Auto & Vehicles                |June 14, 2018     |1.3               |4.0 and up        |\n",
      "|Police Lights, Sirens & Follow Me                                                                           |AUTO_AND_VEHICLES|4.5   |737    |8.9M              |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |June 8, 2018      |2.3.5.1           |4.1 and up        |\n",
      "|Police Detector (Speed Camera Radar)                                                                        |AUTO_AND_VEHICLES|4.3   |3574   |3.9M              |1,000,000+ |Free|0    |Everyone 10+  |Auto & Vehicles                |July 4, 2018      |1.6               |4.0 and up        |\n",
      "|Best Car Wallpapers                                                                                         |AUTO_AND_VEHICLES|4.5   |994    |2.9M              |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |March 27, 2018    |1.6               |4.1 and up        |\n",
      "|Tickets + PDA 2018 Exam                                                                                     |AUTO_AND_VEHICLES|4.9   |197136 |38M               |1,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |July 15, 2018     |8.31              |4.1 and up        |\n",
      "|Pick Your Part Garage                                                                                       |AUTO_AND_VEHICLES|3.9   |142    |32M               |50,000+    |Free|0    |Everyone      |Auto & Vehicles                |July 24, 2018     |1.1.5.0           |6.0 and up        |\n",
      "|PakWheels: Buy & Sell Cars                                                                                  |AUTO_AND_VEHICLES|4.4   |15168  |37M               |1,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |July 27, 2018     |10.0.2            |4.2 and up        |\n",
      "|Supervision service                                                                                         |AUTO_AND_VEHICLES|4.0   |2155   |15M               |500,000+   |Free|0    |Everyone      |Auto & Vehicles                |July 30, 2018     |1.10.3            |5.0 and up        |\n",
      "|Speed Camera Detector - Traffic & Speed Alert                                                               |AUTO_AND_VEHICLES|4.3   |138    |5.4M              |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |July 23, 2018     |1.0               |4.0.3 and up      |\n",
      "|Used car search Goo net whole car Go to net                                                                 |AUTO_AND_VEHICLES|3.7   |5414   |18M               |1,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |July 23, 2018     |3.20.1            |4.4 and up        |\n",
      "|CarMax â€“ Cars for Sale: Search Used Car Inventory                                                           |AUTO_AND_VEHICLES|4.4   |21777  |Varies with device|1,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |August 4, 2018    |Varies with device|Varies with device|\n",
      "|BEST CAR SOUNDS                                                                                             |AUTO_AND_VEHICLES|4.3   |348    |38M               |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |July 29, 2018     |1.0.3             |2.3 and up        |\n",
      "|RST - Sale of cars on the PCT                                                                               |AUTO_AND_VEHICLES|3.2   |250    |1.1M              |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |April 27, 2018    |1.4               |4.0.3 and up      |\n",
      "|AutoScout24 Switzerland â€“ Find your new car                                                                 |AUTO_AND_VEHICLES|4.6   |13372  |Varies with device|1,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |August 3, 2018    |Varies with device|Varies with device|\n",
      "|Zona Azul Digital FÃ¡cil SP CET - OFFICIAL SÃ£o Paulo                                                         |AUTO_AND_VEHICLES|4.6   |7880   |Varies with device|100,000+   |Free|0    |Everyone      |Auto & Vehicles                |May 10, 2018      |4.6.5             |Varies with device|\n",
      "|SMS Park                                                                                                    |AUTO_AND_VEHICLES|4.5   |3617   |7.9M              |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |May 16, 2018      |2.8.2             |4.1 and up        |\n",
      "|SKencar                                                                                                     |AUTO_AND_VEHICLES|3.7   |4806   |35M               |1,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |July 31, 2018     |4.0.3             |4.0.3 and up      |\n",
      "|Fuelio: Gas log & costs                                                                                     |AUTO_AND_VEHICLES|4.6   |65786  |Varies with device|1,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |August 2, 2018    |Varies with device|4.0.3 and up      |\n",
      "|auto fines                                                                                                  |AUTO_AND_VEHICLES|4.6   |31433  |17M               |1,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |August 2, 2018    |1.40              |4.0.3 and up      |\n",
      "|Used car is the first car - used car purchase, used car quotation, dealer information to                    |AUTO_AND_VEHICLES|4.6   |5097   |19M               |1,000,000+ |Free|0    |Everyone      |Auto & Vehicles                |July 23, 2018     |1.5.18            |4.0.3 and up      |\n",
      "|All of the parking lot - National Park application (parking lot search / parking sharing / discount payment)|AUTO_AND_VEHICLES|4.0   |1754   |14M               |500,000+   |Free|0    |Everyone      |Auto & Vehicles                |June 2, 2018      |2.3.4             |4.0 and up        |\n",
      "|Inquiry Fines and Debits of Vehicles                                                                        |AUTO_AND_VEHICLES|4.4   |2680   |2.2M              |500,000+   |Free|0    |Everyone      |Auto & Vehicles                |March 20, 2018    |1.03              |4.0.3 and up      |\n",
      "|Gas Station                                                                                                 |AUTO_AND_VEHICLES|4.0   |1288   |4.5M              |100,000+   |Free|0    |Everyone      |Auto & Vehicles                |April 21, 2018    |2.17              |4.0 and up        |\n",
      "|Hush - Beauty for Everyone                                                                                  |BEAUTY           |4.7   |18900  |17M               |500,000+   |Free|0    |Everyone      |Beauty                         |August 2, 2018    |6.10.1            |5.0 and up        |\n",
      "|ipsy: Makeup, Beauty, and Tips                                                                              |BEAUTY           |4.9   |49790  |14M               |1,000,000+ |Free|0    |Everyone      |Beauty                         |November 9, 2017  |2.3.0             |4.1 and up        |\n",
      "+------------------------------------------------------------------------------------------------------------+-----------------+------+-------+------------------+-----------+----+-----+--------------+-------------------------------+------------------+------------------+------------------+\n",
      "only showing top 100 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read multiple CSV files\n",
    "df = spark.read.csv(path = ['Iris.csv', 'csv_files/google_playstore_data.csv'],header = True)\n",
    "print(df)\n",
    "print(df.printSchema())\n",
    "print(df.show(100,truncate=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb89ce53-28b0-4ee6-9574-ab75f00a5cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_assert_on_driver',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_encryption_enabled',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_serialize_to_jvm',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addArchive',\n",
       " 'addFile',\n",
       " 'addJobTag',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'cancelJobsWithTag',\n",
       " 'clearJobTags',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getCheckpointDir',\n",
       " 'getConf',\n",
       " 'getJobTags',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'listArchives',\n",
       " 'listFiles',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'removeJobTag',\n",
       " 'resources',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setInterruptOnCancel',\n",
       " 'setJobDescription',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46727f66-b645-4a65-ac84-3986977461d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://Rangapavan:4040'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a751ca-bcf0-496c-aff0-cc84ffa888c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
