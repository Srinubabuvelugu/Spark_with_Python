{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321b30bd-20d5-4c20-a0be-325336cebe04",
   "metadata": {},
   "source": [
    "## Types of explode functions\n",
    "1. explode()\n",
    "2. explore_outer()\n",
    "3. posexplode()\n",
    "4. posexplode_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd53a3b3-9427-4099-995a-08052c846cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- knownLanguages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+-------------------+-----------------------------+\n",
      "|name      |knownLanguages     |properties                   |\n",
      "+----------+-------------------+-----------------------------+\n",
      "|James     |[Java, Scala]      |{eye -> brown, hair -> black}|\n",
      "|Michael   |[Spark, Java, NULL]|{eye -> NULL, hair -> brown} |\n",
      "|Robert    |[CSharp, ]         |{eye -> , hair -> red}       |\n",
      "|Washington|NULL               |NULL                         |\n",
      "|Jefferson |[1, 2]             |{}                           |\n",
      "+----------+-------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession and Prepare sample Data\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
    "\n",
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc38163-0794-41db-961c-0a0524b742cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='James', knownLanguages=['Java', 'Scala'], properties={'eye': 'brown', 'hair': 'black'}),\n",
       " Row(name='Michael', knownLanguages=['Spark', 'Java', None], properties={'eye': None, 'hair': 'brown'}),\n",
       " Row(name='Robert', knownLanguages=['CSharp', ''], properties={'eye': '', 'hair': 'red'}),\n",
       " Row(name='Washington', knownLanguages=None, properties=None),\n",
       " Row(name='Jefferson', knownLanguages=['1', '2'], properties={})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f19cc83-4c7d-4a76-b65e-7b1bc9aa01af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- knownLanguages: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "None\n",
      "+---------+--------------+-----------------------------+\n",
      "|name     |knownLanguages|properties                   |\n",
      "+---------+--------------+-----------------------------+\n",
      "|James    |Java          |{eye -> brown, hair -> black}|\n",
      "|James    |Scala         |{eye -> brown, hair -> black}|\n",
      "|Michael  |Spark         |{eye -> NULL, hair -> brown} |\n",
      "|Michael  |Java          |{eye -> NULL, hair -> brown} |\n",
      "|Michael  |NULL          |{eye -> NULL, hair -> brown} |\n",
      "|Robert   |CSharp        |{eye -> , hair -> red}       |\n",
      "|Robert   |              |{eye -> , hair -> red}       |\n",
      "|Jefferson|1             |{}                           |\n",
      "|Jefferson|2             |{}                           |\n",
      "+---------+--------------+-----------------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Apply the explode on knownLanguages column\n",
    "#explode function can convert each element in an array of column into each element as a row\n",
    "from pyspark.sql.functions import explode, col\n",
    "df1 = df.withColumn('knownLanguages',explode(col('knownLanguages')))\n",
    "print(df1.printSchema())\n",
    "print(df1.show(truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73baefc7-03da-468b-a631-b2ea1476032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     name|   col|\n",
      "+---------+------+\n",
      "|    James|  Java|\n",
      "|    James| Scala|\n",
      "|  Michael| Spark|\n",
      "|  Michael|  Java|\n",
      "|  Michael|  NULL|\n",
      "|   Robert|CSharp|\n",
      "|   Robert|      |\n",
      "|Jefferson|     1|\n",
      "|Jefferson|     2|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## selecting the name and knownLanguages as explode columns\n",
    "# explode on array column example\n",
    "df.select(df.name,explode(col('knownLanguages'))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e374c032-6ad7-4ccd-b175-5e8fffd192f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+\n",
      "|   name| key|value|\n",
      "+-------+----+-----+\n",
      "|  James| eye|brown|\n",
      "|  James|hair|black|\n",
      "|Michael| eye| NULL|\n",
      "|Michael|hair|brown|\n",
      "| Robert| eye|     |\n",
      "| Robert|hair|  red|\n",
      "+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode on map column example\n",
    "# on the map column it will create two separate rows for key and value\n",
    "df.select(df.name,explode(col('properties'))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13fe1052-0bd3-453d-8b7a-2921876dccfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      name|   col|\n",
      "+----------+------+\n",
      "|     James|  Java|\n",
      "|     James| Scala|\n",
      "|   Michael| Spark|\n",
      "|   Michael|  Java|\n",
      "|   Michael|  NULL|\n",
      "|    Robert|CSharp|\n",
      "|    Robert|      |\n",
      "|Washington|  NULL|\n",
      "| Jefferson|     1|\n",
      "| Jefferson|     2|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 2. explode_outer() – Create rows for each element in an array or map.\n",
    "# PySpark SQL explode_outer(e: Column) function is used to create a row for each element in the array or map column.\n",
    "# Unlike explode, if the array or map is null or empty, explode_outer returns null.\n",
    "from pyspark.sql.functions import explode_outer\n",
    "df.select(df.name,explode_outer(col('knownLanguages'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fd45e8f-6599-49a8-8b9a-4fe5d952aa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+\n",
      "|      name| key|value|\n",
      "+----------+----+-----+\n",
      "|     James| eye|brown|\n",
      "|     James|hair|black|\n",
      "|   Michael| eye| NULL|\n",
      "|   Michael|hair|brown|\n",
      "|    Robert| eye|     |\n",
      "|    Robert|hair|  red|\n",
      "|Washington|NULL| NULL|\n",
      "| Jefferson|NULL| NULL|\n",
      "+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,explode_outer(col('properties'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acb81f0c-0871-43d4-8432-3e46350044d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----+-----+\n",
      "|   name|pos| key|value|\n",
      "+-------+---+----+-----+\n",
      "|  James|  0| eye|brown|\n",
      "|  James|  1|hair|black|\n",
      "|Michael|  0| eye| NULL|\n",
      "|Michael|  1|hair|brown|\n",
      "| Robert|  0| eye|     |\n",
      "| Robert|  1|hair|  red|\n",
      "+-------+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 3. posexplode() – explode array or map elements to rows\n",
    "# posexplode(e: Column) creates a row for each element in the array.\n",
    "# creates two columns “pos’ to hold the position of the array element and the ‘col’ to hold the actual array value. \n",
    "# For map it will create 3 columns \n",
    "# This will ignore elements that are null or empty. \n",
    "\n",
    "from pyspark.sql.functions import posexplode\n",
    "df.select(df.name,posexplode(col('properties'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb031cb2-615f-47c0-b7b1-8ce391f1f47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|     name|pos|   col|\n",
      "+---------+---+------+\n",
      "|    James|  0|  Java|\n",
      "|    James|  1| Scala|\n",
      "|  Michael|  0| Spark|\n",
      "|  Michael|  1|  Java|\n",
      "|  Michael|  2|  NULL|\n",
      "|   Robert|  0|CSharp|\n",
      "|   Robert|  1|      |\n",
      "|Jefferson|  0|     1|\n",
      "|Jefferson|  1|     2|\n",
      "+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,posexplode(col('knownLanguages'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac2cc082-4422-4053-a5ac-2699cf9b51b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+\n",
      "|      name| pos|   col|\n",
      "+----------+----+------+\n",
      "|     James|   0|  Java|\n",
      "|     James|   1| Scala|\n",
      "|   Michael|   0| Spark|\n",
      "|   Michael|   1|  Java|\n",
      "|   Michael|   2|  NULL|\n",
      "|    Robert|   0|CSharp|\n",
      "|    Robert|   1|      |\n",
      "|Washington|NULL|  NULL|\n",
      "| Jefferson|   0|     1|\n",
      "| Jefferson|   1|     2|\n",
      "+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 4. posexplode_outer() – explode array or map columns to rows.\n",
    "# It is the combination of explode_outer and position\n",
    "# # This will show elements that are null or empty. \n",
    "from pyspark.sql.functions import posexplode_outer\n",
    "df.select(df.name,posexplode_outer(col('knownLanguages'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d227df7-61ef-466f-9486-0993b5dd45cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-----+\n",
      "|      name| pos| key|value|\n",
      "+----------+----+----+-----+\n",
      "|     James|   0| eye|brown|\n",
      "|     James|   1|hair|black|\n",
      "|   Michael|   0| eye| NULL|\n",
      "|   Michael|   1|hair|brown|\n",
      "|    Robert|   0| eye|     |\n",
      "|    Robert|   1|hair|  red|\n",
      "|Washington|NULL|NULL| NULL|\n",
      "| Jefferson|NULL|NULL| NULL|\n",
      "+----------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,posexplode_outer(col('properties'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95836e0c-6040-4fee-af1f-518a24a753b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- knownLanguages: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "None\n",
      "+---------+--------------+-----------------------------+\n",
      "|name     |knownLanguages|properties                   |\n",
      "+---------+--------------+-----------------------------+\n",
      "|James    |Java          |{eye -> brown, hair -> black}|\n",
      "|James    |Scala         |{eye -> brown, hair -> black}|\n",
      "|Michael  |Spark         |{eye -> NULL, hair -> brown} |\n",
      "|Michael  |Java          |{eye -> NULL, hair -> brown} |\n",
      "|Michael  |NULL          |{eye -> NULL, hair -> brown} |\n",
      "|Robert   |CSharp        |{eye -> , hair -> red}       |\n",
      "|Robert   |              |{eye -> , hair -> red}       |\n",
      "|Jefferson|1             |{}                           |\n",
      "|Jefferson|2             |{}                           |\n",
      "+---------+--------------+-----------------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn('knownLanguages',explode(col('knownLanguages')))\n",
    "print(df2.printSchema())\n",
    "print(df2.show(truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75a583c4-9afd-46e4-8d9f-d6ea11cf1f28",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UDTF_ALIAS_NUMBER_MISMATCH] The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF. Expected 3 aliases, but got properties. Please ensure that the number of aliases provided matches the number of columns output by the UDTF.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df3 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m'\u001b[39m,posexplode_outer(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df3\u001b[38;5;241m.\u001b[39mprintSchema())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df3\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:5174\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m   5170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5171\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5172\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5173\u001b[0m     )\n\u001b[1;32m-> 5174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UDTF_ALIAS_NUMBER_MISMATCH] The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF. Expected 3 aliases, but got properties. Please ensure that the number of aliases provided matches the number of columns output by the UDTF."
     ]
    }
   ],
   "source": [
    "df3 = df.withColumn('properties',posexplode_outer(col('properties')))\n",
    "print(df3.printSchema())\n",
    "print(df3.show(truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632025a-678a-4c2b-83f9-672b5b6c943f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
